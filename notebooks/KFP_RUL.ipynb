{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this value with the value of the KFP host name\n",
    "KFP_HOST_NAME = 'https://31a528101f3f8d80-dot-us-central2.pipelines.googleusercontent.com/'\n",
    "# number of latest days to holdout for validation\n",
    "holdout_engine = 62\n",
    "# number of random iterations for random search hyperparameter optimization\n",
    "random_iterations = 3\n",
    "# Number of trees in random forest\n",
    "n_estimators = [150, 250, 300, 400]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5, 10, 50, 100, None]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 3, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host=KFP_HOST_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(raw_data_path: str, cleaned_data_path: str) -> str:\n",
    "    '''Hier moet de data import komen'''\n",
    "    import pandas as pd\n",
    "    \n",
    "    #raw data import\n",
    "    data = pd.read_csv(raw_data_path, sep=\" \", header=None)\n",
    "    \n",
    "    data.columns = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21', 'NA', 'NA']\n",
    "    del data['NA']\n",
    "\n",
    "    # Creating and adding the RUL to the dataframe\n",
    "    RUL_list = []\n",
    "    for engine in set(data['engine_id']):\n",
    "        max_cycle = data.loc[data['engine_id'] == engine].cycle.max()\n",
    "    \n",
    "        for cycle in list(data.loc[data['engine_id'] == engine].cycle):\n",
    "            RUL_list.append(max_cycle - cycle +1)    \n",
    "        \n",
    "\n",
    "    data.insert(2, 'RUL', RUL_list)\n",
    "    data.to_parquet(cleaned_data_path, compression='GZIP')\n",
    "    \n",
    "    return cleaned_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clean_data_op = comp.create_component_from_func(\n",
    "    get_clean_data, output_component_file = 'get_clean_data.yaml', packages_to_install=['fastparquet', 'fsspec', 'gcsfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_processing(cleaned_data_path: str, feature_data_path: str) -> str:\n",
    "    import pandas as pd\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    data = pd.read_parquet(cleaned_data_path)\n",
    "    \n",
    "    engine = data.iloc[:,0].to_list()\n",
    "    cycle = data.iloc[:,1].to_list()\n",
    "\n",
    "    # Clustering the data\n",
    "    X_cluster = data[['setting1', 'setting2', 'setting3']]\n",
    "\n",
    "    # creates the clusters\n",
    "    kmeans = KMeans(n_clusters=3).fit(X_cluster)\n",
    "    data['settings_clusters'] = kmeans.predict(X_cluster)\n",
    "\n",
    "    features = data.columns[3:-1]\n",
    "    for feature in features:\n",
    "        # Creating min, max and delta variables\n",
    "        data['max_' + feature] = data.groupby('engine_id')[feature].cummax()\n",
    "        data['min_' + feature] = data.groupby('engine_id')[feature].cummin()\n",
    "\n",
    "        data['delta_' + feature] = data.groupby('engine_id')[feature].diff()\n",
    "        data['delta_' + feature].fillna(0, inplace=True)\n",
    "\n",
    "    \n",
    "    data.to_parquet(feature_data_path, compression='GZIP')\n",
    "    \n",
    "    print('Created an saved features.')\n",
    "    \n",
    "    return feature_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_processing_op = comp.create_component_from_func(\n",
    "    feature_processing, output_component_file = 'feature_processing.yaml', packages_to_install=['fastparquet', 'fsspec', 'gcsfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vanilla_gbr(feature_data_path: str, vanilla_model_store_path: str, holdout_engine: int) -> str:\n",
    "    import pandas as pd\n",
    "    import _pickle as cPickle\n",
    "    from google.cloud import storage\n",
    "    from urlib.parse import urlparse\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import ensemble\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    data = pd.read_parquet(feature_data_path)\n",
    "    \n",
    "    RUL_df = data.loc[data.engine_id != holdout_engine].iloc[:,2:].copy()\n",
    "    \n",
    "    labels = RUL_df['RUL']\n",
    "    features = RUL_df.iloc[:,1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    gbr_non_opt = ensemble.GradientBoostingRegressor()\n",
    "    gbr_non_opt.fit(X_train, y_train)\n",
    "    \n",
    "    pred_non_opt = gbr_non_opt.predict(X_test)\n",
    "    print('MAE: %s' % metrics.mean_absolute_error(y_test, pred_non_opt))\n",
    "    print('MSE: %s' % metrics.mean_squared_error(y_test, pred_non_opt))\n",
    "    \n",
    "    temp_model_path = '/tmp/model.pickle'\n",
    "        \n",
    "    with open(temp_model_path, 'wb') as f:\n",
    "        cPickle.dump(gbr_non_opt, f, -1)\n",
    "        \n",
    "    parse = urlparse(url=vanilla_model_store_path, allow_fragments = False)\n",
    "    if parse.path[0] =='/':\n",
    "        model_path = parse.path[1:]\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(parse.netloc)\n",
    "    model = bucket.blob(model_path)\n",
    "    model.upload_from_filename(temp_model_path)\n",
    "        \n",
    "    return vanilla_model_store_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vanilla_gbr_op = comp.create_component_from_func(\n",
    "    train_vanilla_gbr, output_component_file = 'train_vanilla_gbr.yaml', packages_to_install=['fastparquet', 'fsspec', 'gcsfs', 'scikit-learn', 'google-cloud-storage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_tune_train_gbr(feature_data_path: str, tuned_model_store_path: str,\n",
    "                       holdout_engine: int, random_iterations: int, \n",
    "                       random_params: str) -> str:\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import _pickle as cPickle\n",
    "    from google.cloud import storage\n",
    "    from urlib.parse import urlparse\n",
    "    from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "    from sklearn import ensemble\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    data = pd.read_parquet(feature_data_path)\n",
    "    \n",
    "    RUL_df = data.loc[data.engine_id != holdout_engine].iloc[:,2:].copy()\n",
    "    \n",
    "    labels = RUL_df['RUL']\n",
    "    features = RUL_df.iloc[:,1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    random_grid = json.loads(random_params)\n",
    "    \n",
    "    gbr = ensemble.GradientBoostingRegressor()\n",
    "    gbr_random = RandomizedSearchCV(estimator = gbr, param_distributions = random_grid, n_iter = 10, cv = 3, verbose = 2)\n",
    "    gbr_random.fit(X_train, y_train)\n",
    "\n",
    "    val_pred_random = gbr_random.predict(X_test)\n",
    "    MAE_random = metrics.mean_absolute_error(y_test, val_pred_random)\n",
    "    MSE_random = metrics.mean_squared_error(y_test, val_pred_random)\n",
    "    print('MAE: %s' % MAE_random)\n",
    "    print('MSE: %s' % MSE_random)\n",
    "        \n",
    "    temp_model_path = '/tmp/model.pickle'\n",
    "        \n",
    "    with open(temp_model_path, 'wb') as f:\n",
    "        cPickle.dump(gbr_random.best_estimator_, f, -1)\n",
    "        \n",
    "    parse = urlparse(url=tuned_model_store_path, allow_fragments = False)\n",
    "    if parse.path[0] =='/':\n",
    "        model_path = parse.path[1:]\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(parse.netloc)\n",
    "    model = bucket.blob(model_path)\n",
    "    model.upload_from_filename(temp_model_path)\n",
    "    \n",
    "    return tuned_model_store_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_tune_train_gbr_op = comp.create_component_from_func(\n",
    "    hyp_tune_train_gbr, output_component_file = 'hyp_tune_train_gbr.yaml', packages_to_install=['fastparquet', 'fsspec', 'gcsfs', 'scikit-learn', 'google-cloud-storage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(feature_data_path: str, vanilla_model_store_path: str, tuned_model_store_path: str, holdout_engine: int) -> None:\n",
    "    '''Evaluate different models on holdout dataset to see which model performs the best'''\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    from datetime import datetime, timedelta\n",
    "    import _pickle as cPickle # save ML model\n",
    "    from google.cloud import storage # save the model to GCS\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from urllib.parse import urlparse\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    # read dataframe\n",
    "    complete_df = pd.read_parquet(feature_data_path)\n",
    "    \n",
    "    # this will be our holdout set for validation\n",
    "    holdout_df = complete_df.loc[complete_df.engine_id == holdout_engine].iloc[:,2:].copy()\n",
    "    \n",
    "    # get x and y\n",
    "    x_val, y_val = holdout_df.drop('RUL', axis=1), holdout_df['RUL']\n",
    "    \n",
    "    def get_mae(model_path):\n",
    "        '''this function evaluates a model on our holdout dataset given just the model path'''\n",
    "        parse = urlparse(url=model_path, allow_fragments=False)\n",
    "\n",
    "        if parse.path[0] =='/':\n",
    "            model_path = parse.path[1:]\n",
    "\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(parse.netloc)\n",
    "        blob = bucket.get_blob(model_path)\n",
    "        if blob is None:\n",
    "            raise AttributeError('No files to download') \n",
    "        model_bytestream = BytesIO(blob.download_as_string())\n",
    "        model = cPickle.load(model_bytestream)\n",
    "        predictions = model.predict(x_val)\n",
    "        return mean_absolute_error(y_val, predictions)\n",
    "    \n",
    "    Models = namedtuple('Model', 'type score path')\n",
    "    m_list = list()\n",
    "    \n",
    "    vanilla_mae = get_mae(vanilla_model_store_path)\n",
    "    m_list.append(Models('vanilla', vanilla_mae, vanilla_model_store_path))\n",
    "    \n",
    "    tuned_mae = get_mae(tuned_model_store_path)\n",
    "    m_list.append(Models('tuned', tuned_mae, tuned_model_store_path))\n",
    "    \n",
    "    max_score = max([model.score for model in m_list])\n",
    "    max_score_index = [model.score for model in m_list].index(max_score)\n",
    "    print('Best Model: ', m_list[max_score_index])\n",
    "    path = m_list[max_score_index].path\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component\n",
    "eval_models_op = comp.create_component_from_func(\n",
    "    eval_models, output_component_file='eval_models.yaml', packages_to_install=['scikit-learn', 'fastparquet', 'fsspec', 'gcsfs', 'google-cloud-storage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://31a528101f3f8d80-dot-us-central2.pipelines.googleusercontent.com//#/experiments/details/363dc06c-21a7-4b0f-b771-6bbfd36c2a0e\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://31a528101f3f8d80-dot-us-central2.pipelines.googleusercontent.com//#/runs/details/35264ddf-1de2-4026-a5de-7ddffce8f604\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=35264ddf-1de2-4026-a5de-7ddffce8f604)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name='RUL gbr',\n",
    "    description='Predicting the Remaining Usefull Lifetime of aircraft engines.'\n",
    ")\n",
    "def RUL_pipeline(raw_data_path, cleaned_data_path, feature_data_path, vanilla_model_store_path, tuned_model_store_path, holdout_engine, random_iterations, random_params):\n",
    "    \n",
    "    get_clean_data_task = get_clean_data_op(raw_data_path, cleaned_data_path)\n",
    "    feature_processing_task = feature_processing_op(get_clean_data_task.output, feature_data_path)\n",
    "    train_vanilla_gbr_task = train_vanilla_gbr_op(feature_processing_task.output, vanilla_model_store_path, holdout_engine)\n",
    "    hyp_tune_train_gbr_task = hyp_tune_train_gbr_op(feature_processing_task.output, tuned_model_store_path, holdout_engine, random_iterations, random_params)\n",
    "    eval_models_task = eval_models_op(feature_processing_task.output, train_vanilla_gbr_task.output, hyp_tune_train_gbr_task.output, holdout_engine)\n",
    "    \n",
    "random_grid = {'n_estimators' : n_estimators,\n",
    "               'max_depth' : max_depth,\n",
    "               'max_features' : max_features,\n",
    "               'min_samples_split' : min_samples_split,\n",
    "               'min_samples_leaf' : min_samples_leaf\n",
    "}\n",
    "    \n",
    "arguments = {'raw_data_path': 'gs://de_a3/raw/DataTrain.txt',\n",
    "            'cleaned_data_path' : 'gs://de_a3/cleaned/cleaned_data.parquet',\n",
    "            'feature_data_path' : 'gs://de_a3/feature_store/RUL_features.parquet',\n",
    "            'vanilla_model_store_path' : 'gs://de_a3/model_store/vanilla/vanilla_gbr.pickle',\n",
    "            'tuned_model_store_path' : 'gs://de_a3/model_store/tuned/tuned_gbr.pickle',\n",
    "            'holdout_engine': holdout_engine,\n",
    "            'random_iterations': random_iterations,\n",
    "            'random_params': random_grid,\n",
    "}\n",
    "\n",
    "    \n",
    "client.create_run_from_pipeline_func(RUL_pipeline, arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
