{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this value with the value of the KFP host name\n",
    "KFP_HOST_NAME = 'https://31a528101f3f8d80-dot-us-central2.pipelines.googleusercontent.com/'\n",
    "# number of latest days to holdout for validation\n",
    "holdout_engine = 62\n",
    "# number of random iterations for random search hyperparameter optimization\n",
    "random_iterations = 3\n",
    "# Number of trees in random forest\n",
    "n_estimators = [150, 250, 300, 400]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5, 10, 50, 100, None]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 3, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host=KFP_HOST_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(raw_data_path: str, cleaned_data_path: str) -> str:\n",
    "    '''Hier moet de data import komen'''\n",
    "    import pandas as pd\n",
    "    \n",
    "    #raw data import\n",
    "    data = pd.read_csv(raw_data_path, sep=\" \", header=None)\n",
    "    \n",
    "    data.columns = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21', 'NA', 'NA']\n",
    "    del data['NA']\n",
    "\n",
    "    # Creating and adding the RUL to the dataframe\n",
    "    RUL_list = []\n",
    "    for engine in set(data['engine_id']):\n",
    "        max_cycle = data.loc[data['engine_id'] == engine].cycle.max()\n",
    "    \n",
    "        for cycle in list(data.loc[data['engine_id'] == engine].cycle):\n",
    "            RUL_list.append(max_cycle - cycle +1)    \n",
    "        \n",
    "\n",
    "    data.insert(2, 'RUL', RUL_list)\n",
    "    data.to_parquet(cleaned_data_path, compression='GZIP')\n",
    "    \n",
    "    return cleaned_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clean_data_op = comp.create_component_from_func(\n",
    "    get_clean_data, output_component_file = 'get_clean_data.yaml', packages_to_install=['fastparquet', 'fsspec', 'gcfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_processing(cleaned_data_path: str, feature_data_path: str) -> str:\n",
    "    import pandas as pd\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    data = pd.read_parquet(cleaned_data_path)\n",
    "    \n",
    "    engine = data.iloc[:,0].to_list()\n",
    "    cycle = data.iloc[:,1].to_list()\n",
    "\n",
    "    # Clustering the data\n",
    "    X_cluster = data[['setting1', 'setting2', 'setting3']]\n",
    "\n",
    "    # creates the clusters\n",
    "    kmeans = KMeans(n_clusters=3).fit(X_cluster)\n",
    "    data['settings_clusters'] = kmeans.predict(X_cluster)\n",
    "\n",
    "    features = data.columns[3:-1]\n",
    "    for feature in features:\n",
    "        # Creating min, max and delta variables\n",
    "        data['max_' + feature] = data.groupby('engine_id')[feature].cummax()\n",
    "        data['min_' + feature] = data.groupby('engine_id')[feature].cummin()\n",
    "\n",
    "        data['delta_' + feature] = data.groupby('engine_id')[feature].diff()\n",
    "        data['delta_' + feature].fillna(0, inplace=True)\n",
    "\n",
    "    \n",
    "    data.to_parquet(feature_data_path, compression='GZIP')\n",
    "    \n",
    "    print('Created an saved features.')\n",
    "    \n",
    "    return feature_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_processing_op = comp.create_component_from_func(\n",
    "    feature_processing, output_component_file = 'feature_processing.yaml', packages_to_install=['fastparquet', 'fsspec', 'gcfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vanilla_gbr(feature_data_path: str, vanilla_model_store_path: str, holdout_engine: int) -> None:\n",
    "    import pandas as pd\n",
    "    import _pickle as cPickle\n",
    "    from google.cloud import storage\n",
    "    from urlib.parse import urlparse\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import ensemble\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    data = pd.read_parquet(feature_data_path)\n",
    "    \n",
    "    RUL_df = data.loc[data.engine_id != holdout_engine].iloc[:,2:].copy()\n",
    "    \n",
    "    labels = RUL_df['RUL']\n",
    "    features = RUL_df.iloc[:,1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    gbr_non_opt = ensemble.GradientBoostingRegressor()\n",
    "    gbr_non_opt.fit(X_train, y_train)\n",
    "    \n",
    "    pred_non_opt = gbr_non_opt.predict(X_test)\n",
    "    print('MAE: %s' % metrics.mean_absolute_error(y_test, pred_non_opt))\n",
    "    print('MSE: %s' % metrics.mean_squared_error(y_test, pred_non_opt))\n",
    "    \n",
    "    with open('/tmp/model.pickle', 'wb') as f:\n",
    "        cPickle.dump(gbr_non_opt, f, -1)\n",
    "        \n",
    "    parse = urlparse(url=vanilla_model_store_path, allow_fragments = False)\n",
    "    if parse.path[0] == '/':\n",
    "        model_path = parse.path[1:]\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(parse.netloc)\n",
    "    blob = bucket.blob(model_path)\n",
    "    blob.upload_from_filename('/tmp/model.pickle')\n",
    "        \n",
    "    return vanilla_model_store_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vanilla_gbr_op = comp.create_component_from_func(\n",
    "    train_vanilla_gbr, output_component_file = 'train_vanilla_gbr.yaml', packages_to_install=['fastparquet', 'fsspec', 'gcfs', 'scikit-learn', 'google-cloud-storage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_tune_train_gbr(feature_data_path: str, tuned_model_store_path: str,\n",
    "                       holdout_engine: int, random_iterations: int, \n",
    "                       random_params: str) -> str:\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import _pickle as cPickle\n",
    "    from google.cloud import storage\n",
    "    from urlib.parse import urlparse\n",
    "    from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "    from sklearn import ensemble\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    data = pd.read_parquet(feature_data_path)\n",
    "    \n",
    "    RUL_df = data.loc[data.engine_id != holdout_engine].iloc[:,2:].copy()\n",
    "    \n",
    "    labels = RUL_df['RUL']\n",
    "    features = RUL_df.iloc[:,1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    random_grid = json.loads(random_params)\n",
    "    \n",
    "    gbr = ensemble.GradientBoostingRegressor()\n",
    "    gbr_random = RandomizedSearchCV(estimator = gbr, param_distributions = random_grid, n_iter = 10, cv = 3, verbose = 2)\n",
    "    gbr_random.fit(X_train, y_train)\n",
    "\n",
    "    val_pred_random = gbr_random.predict(X_test)\n",
    "    MAE_random = metrics.mean_absolute_error(y_test, val_pred_random)\n",
    "    MSE_random = metrics.mean_squared_error(y_test, val_pred_random)\n",
    "    print('MAE: %s' % MAE_random)\n",
    "    print('MSE: %s' % MSE_random)\n",
    "        \n",
    "    temp_model_path = '/tmp/model.pickle'\n",
    "        \n",
    "    with open(temp_model_path, 'wb') as f:\n",
    "        cPickle.dump(gbr_random.best_estimator_, f, -1)\n",
    "        \n",
    "    parse = urlparse(url=tuned_model_store_path, allow_fragments = False)\n",
    "    if parse.path[0] =='/':\n",
    "        model_path = parse.path[1:]\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(parse.netloc)\n",
    "    model = bucket.blob(model_path)\n",
    "    model.upload_from_filename(temp_model_path)\n",
    "    \n",
    "    return tuned_model_store_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_tune_train_gbr_op = comp.create_component_from_func(\n",
    "    hyp_tune_train_gbr, output_component_file = 'hyp_tune_train_gbr.yaml', packages_to_install=['fastparquet', 'fsspec', 'gcfs', 'scikit-learn', 'google-cloud-storage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(feature_data_path: str, vanilla_model_store_path, tuned_model_store_path: str, holdout_engine: int) -> None:\n",
    "    '''Evaluate different models on holdout dataset to see which model performs the best'''\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    from datetime import datetime, timedelta\n",
    "    import _pickle as cPickle # save ML model\n",
    "    from google.cloud import storage # save the model to GCS\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from urllib.parse import urlparse\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    # read dataframe\n",
    "    complete_df = pd.read_parquet(feature_data_path)\n",
    "    \n",
    "    # this will be our holdout set for validation\n",
    "    holdout_df = complete_df.loc[complete_df.engine_id == holdout_engine].iloc[:,2:].copy()\n",
    "    \n",
    "    # get x and y\n",
    "    x_val, y_val = holdout_df.drop('RUL', axis=1), holdout_df['RUL']\n",
    "    \n",
    "    def get_mae(model_path):\n",
    "        '''this function evaluates a model on our holdout dataset given just the model path'''\n",
    "        parse = urlparse(url=model_path, allow_fragments=False)\n",
    "\n",
    "        if parse.path[0] =='/':\n",
    "            model_path = parse.path[1:]\n",
    "\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(parse.netloc)\n",
    "        blob = bucket.get_blob(model_path)\n",
    "        if blob is None:\n",
    "            raise AttributeError('No files to download') \n",
    "        model_bytestream = BytesIO(blob.download_as_string())\n",
    "        model = cPickle.load(model_bytestream)\n",
    "        predictions = model.predict(x_val)\n",
    "        return mean_absolute_error(y_val, predictions)\n",
    "    \n",
    "    Models = namedtuple('Model', 'type score path')\n",
    "    m_list = list()\n",
    "    \n",
    "    vanilla_mae = get_mae(vanilla_model_store_path)\n",
    "    m_list.append(Models('vanilla', vanilla_mae, vanilla_model_store_path))\n",
    "    \n",
    "    tuned_mae = get_mae(tuned_model_store_path)\n",
    "    m_list.append(Models('tuned', tuned_mae, tuned_model_store_path))\n",
    "    \n",
    "    max_score = max([model.score for model in m_list])\n",
    "    max_score_index = [model.score for model in m_list].index(max_score)\n",
    "    print('Best Model: ', m_list[max_score_index])\n",
    "    path = m_list[max_score_index].path\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component\n",
    "eval_models_op = comp.create_component_from_func(\n",
    "    eval_models, output_component_file='eval_models.yaml', packages_to_install=['scikit-learn', 'fastparquet', 'fsspec', 'gcsfs', 'google-cloud-storage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This task has multiple outputs. Use `task.outputs[<output name>]` dictionary to refer to the one you need.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-693977124e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run_from_pipeline_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRUL_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/_client.py\u001b[0m in \u001b[0;36mcreate_run_from_pipeline_func\u001b[0;34m(self, pipeline_func, arguments, run_name, experiment_name, pipeline_conf, namespace)\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m       \u001b[0mpipeline_package_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pipeline.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m       \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_package_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run_from_pipeline_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_package_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, pipeline_func, package_path, type_check, pipeline_conf)\u001b[0m\n\u001b[1;32m    921\u001b[0m           \u001b[0mpipeline_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m           \u001b[0mpipeline_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_conf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m           package_path=package_path)\n\u001b[0m\u001b[1;32m    924\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE_CHECK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_check_old_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_create_and_write_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf, package_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mpipeline_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0mparams_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         pipeline_conf)\n\u001b[0m\u001b[1;32m    978\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0m_validate_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_create_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdsl_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m       \u001b[0mpipeline_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0mpipeline_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_conf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdsl_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;31m# Configuration passed to the compiler is overriding. Unfortunately, it's not trivial to detect whether the dsl_pipeline.conf was ever modified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-693977124e95>\u001b[0m in \u001b[0;36mRUL_pipeline\u001b[0;34m(raw_data_path, cleaned_data_path, feature_data_path, vanilla_model_store_path, tuned_model_store_path, holdout_engine, random_iterations, random_params)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_vanilla_gbr_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vanilla_gbr_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_processing_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvanilla_model_store_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholdout_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mhyp_tune_train_gbr_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyp_tune_train_gbr_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_processing_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuned_model_store_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholdout_engine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0meval_models_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_models_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_processing_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_vanilla_gbr_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp_tune_train_gbr_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholdout_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m random_grid = {'n_estimators' : n_estimators,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/components/_dynamic.py\u001b[0m in \u001b[0;36mEval models\u001b[0;34m(feature_data_path, vanilla_model_store_path, tuned_model_store_path, holdout_engine)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpass_locals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: F821 TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpass_locals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/components/_components.py\u001b[0m in \u001b[0;36mcreate_task_object_from_component_and_pythonic_arguments\u001b[0;34m(pythonic_arguments)\u001b[0m\n\u001b[1;32m    331\u001b[0m         arguments = {\n\u001b[1;32m    332\u001b[0m             \u001b[0mpythonic_name_to_input_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margument_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margument_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0margument_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpythonic_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margument_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_DefaultValue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Skipping passing arguments for optional values that have not been overridden.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         }\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/components/_components.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mpythonic_name_to_input_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margument_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margument_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0margument_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpythonic_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margument_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_DefaultValue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Skipping passing arguments for optional values that have not been overridden.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         }\n\u001b[1;32m    336\u001b[0m         return _create_task_object_from_component_and_arguments(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/dsl/_container_op.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0m_MultipleOutputsError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/dsl/_container_op.py\u001b[0m in \u001b[0;36mraise_error\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1232\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This task has multiple outputs. Use `task.outputs[<output name>]` dictionary to refer to the one you need.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This task has multiple outputs. Use `task.outputs[<output name>]` dictionary to refer to the one you need."
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name='RUL gbr',\n",
    "    description='Predicting the Remaining Usefull Lifetime of aircraft engines.'\n",
    ")\n",
    "def RUL_pipeline(raw_data_path, cleaned_data_path, feature_data_path, vanilla_model_store_path, tuned_model_store_path, holdout_engine, random_iterations, random_params):\n",
    "    \n",
    "    get_clean_data_task = get_clean_data_op(raw_data_path, cleaned_data_path)\n",
    "    feature_processing_task = feature_processing_op(get_clean_data_task.output, feature_data_path)\n",
    "    train_vanilla_gbr_task = train_vanilla_gbr_op(feature_processing_task.output, vanilla_model_store_path, holdout_engine)\n",
    "    hyp_tune_train_gbr_task = hyp_tune_train_gbr_op(feature_processing_task.output, tuned_model_store_path, holdout_engine, random_iterations, random_params)\n",
    "    eval_models_task = eval_models_op(feature_processing_task.output, train_vanilla_gbr_task.output, hyp_tune_train_gbr_task.output, holdout_engine)\n",
    "    \n",
    "random_grid = {'n_estimators' : n_estimators,\n",
    "               'max_depth' : max_depth,\n",
    "               'max_features' : max_features,\n",
    "               'min_samples_split' : min_samples_split,\n",
    "               'min_samples_leaf' : min_samples_leaf\n",
    "}\n",
    "    \n",
    "arguments = {'raw_data_path': 'gs://DE_A3/raw/DataTrain.txt',\n",
    "            'cleaned_data_path' : 'gs://DE_A3/cleaned/cleaned_data.parquet',\n",
    "            'feature_data_path' : 'gs://DE_A3/feature_store/RUL_features.parquet',\n",
    "            'vanilla_model_store_path' : 'gs://DE_A3/model_store/vanilla/vanilla_gbr.pickle',\n",
    "            'tuned_model_store_path' : 'gs://DE_A3/model_store/tuned/tuned_gbr.pickle',\n",
    "            'holdout_engine': holdout_engine,\n",
    "            'random_iterations': random_iterations,\n",
    "            'random_params': random_grid,\n",
    "}\n",
    "\n",
    "    \n",
    "client.create_run_from_pipeline_func(RUL_pipeline, arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
